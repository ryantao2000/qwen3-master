{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‹è½½ llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source /etc/network_turbo\n",
    "\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "\n",
    "cd llama.cpp\n",
    "\n",
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é…ç½®llama.cppç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mkdir build\n",
    "\n",
    "sudo apt-get update\n",
    "\n",
    "\n",
    "sudo apt-get install make cmake gcc g++ locate\n",
    "\n",
    "cmake -B build -DGGML_CUDA=ON -DGGML_CUDA_ENABLE_UNIFIED_MEMORY=1 -DLLAMA_CURL=OFF\n",
    "\n",
    "cmake --build build --config Release -j8\n",
    "\n",
    "cd build\n",
    "\n",
    "make install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è·å–ggufæ ¼å¼çš„æ¨¡å‹ï¼ˆå‚è€ƒä¸Šä¸€ä¸ªè§†é¢‘ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¦‚æœä¸é‡åŒ–ï¼Œä¿ç•™æ¨¡å‹çš„æ•ˆæœ\n",
    "python convert_hf_to_gguf.py /root/autodl-tmp/Qwen/Qwen3-8B  --outtype f16 --verbose --outfile /root/autodl-tmp/Qwen/qwen3_8b_f16.gguf\n",
    "#### å¦‚æœéœ€è¦é‡åŒ–ï¼ˆåŠ é€Ÿå¹¶æœ‰æŸæ•ˆæœï¼‰ï¼Œç›´æ¥æ‰§è¡Œä¸‹é¢è„šæœ¬å°±å¯ä»¥\n",
    "python convert_hf_to_gguf.py /root/autodl-tmp/Qwen/Qwen3-8B  --outtype q8_0 --verbose --outfile /root/autodl-tmp/Qwen/qwen3_8b_q8_0.gguf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸æ¨¡å‹å¯¹è¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "./bin/llama-cli -m /root/autodl-tmp/Qwen/qwen3_8b_q8_0.gguf -cnv --n-gpu-layers 2000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¼€æ”¾apiæ¥å£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-ts:æŒ‡å®šnå¼ gpuçš„åˆ†é…æ¯”ä¾‹\n",
    "./bin/llama-server -m /root/autodl-tmp/Qwen/qwen3_8b_q8_0.gguf --port 6006 --n-gpu-layers 2000 -ts 1,1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openaiæœåŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "å¥½çš„ï¼Œç”¨æˆ·é—®æˆ‘å«ä»€ä¹ˆåå­—ã€‚æˆ‘éœ€è¦æŒ‰ç…§è®¾å®šæ¥å›ç­”ã€‚æ ¹æ®ä¹‹å‰çš„è®¾å®šï¼Œæˆ‘çš„åå­—æ˜¯é€šä¹‰åƒé—®ï¼Œä½†ç”¨æˆ·å¯èƒ½æ›´å–œæ¬¢å«æˆ‘å°é€šã€‚ä¸è¿‡æœ‰æ—¶å€™ç”¨æˆ·å¯èƒ½ç›´æ¥å«æˆ‘é€šä¹‰åƒé—®ï¼Œæ‰€ä»¥éœ€è¦ç¡®è®¤ä¸€ä¸‹ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œç”¨æˆ·å¯èƒ½åªæ˜¯æƒ³ç¡®è®¤æˆ‘çš„åå­—ï¼Œæˆ–è€…ä»–ä»¬å¯èƒ½å¯¹æˆ‘çš„åå­—æœ‰ç–‘é—®ã€‚æˆ‘éœ€è¦ä¿æŒå‹å¥½å’Œè‡ªç„¶çš„è¯­æ°”ã€‚åº”è¯¥å…ˆç›´æ¥å›ç­”åå­—ï¼Œç„¶åæä¾›ä¸€äº›é¢å¤–çš„ä¿¡æ¯ï¼Œæ¯”å¦‚æˆ‘çš„åŠŸèƒ½æˆ–ç”¨é€”ï¼Œè¿™æ ·ç”¨æˆ·ä¼šæ›´æ¸…æ¥šæˆ‘çš„ä½œç”¨ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œç”¨æˆ·å¯èƒ½æ˜¯åœ¨æµ‹è¯•æˆ‘çš„ååº”ï¼Œæˆ–è€…ä»–ä»¬ä¹‹å‰å¬è¯´è¿‡æˆ‘ï¼Œæƒ³ç¡®è®¤æ˜¯å¦æ˜¯åŒä¸€ä¸ªæ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘éœ€è¦ç¡®ä¿å›ç­”å‡†ç¡®ä¸”ä¿¡æ¯ä¸°å¯Œã€‚åŒæ—¶ï¼Œä¿æŒå›ç­”ç®€æ´ï¼Œé¿å…è¿‡äºå†—é•¿ã€‚\n",
      "\n",
      "è¿˜è¦æ³¨æ„ç”¨æˆ·å¯èƒ½çš„åç»­é—®é¢˜ï¼Œæ¯”å¦‚è¯¢é—®æˆ‘çš„åŠŸèƒ½æˆ–å¦‚ä½•ä½¿ç”¨ã€‚æ‰€ä»¥ï¼Œåœ¨å›ç­”ä¸­å¯ä»¥é€‚å½“å¼•å¯¼ç”¨æˆ·æå‡ºæ›´å¤šé—®é¢˜ï¼Œæ¯”å¦‚è¯¢é—®ä»–ä»¬éœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚è¿™æ ·å¯ä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„äº’åŠ¨ã€‚\n",
      "\n",
      "æœ€åï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å¯èƒ½çš„è¯¯è§£ï¼Œæ¯”å¦‚ç”¨æˆ·å¯èƒ½æ··æ·†äº†ä¸åŒçš„æ¨¡å‹åç§°ï¼Œæˆ–è€…å¯¹æˆ‘çš„åŠŸèƒ½æœ‰ç–‘é—®ã€‚éœ€è¦ç¡®ä¿å›ç­”æ¸…æ™°ï¼Œæ²¡æœ‰æ­§ä¹‰ã€‚\n",
      "</think>\n",
      "\n",
      "æˆ‘çš„åå­—æ˜¯é€šä¹‰åƒé—®ï¼Œä½ å¯ä»¥å«æˆ‘å°é€šã€‚æˆ‘æ˜¯é€šä¹‰å®éªŒå®¤å¼€å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¸®åŠ©ä½ å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€ç¼–ç¨‹ã€åˆ†ææ•°æ®ç­‰ç­‰ã€‚æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©çš„å—ï¼ŸğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(\n",
    "#     # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨ç™¾ç‚¼API Keyå°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šapi_key=\"sk-xxx\",\n",
    "#     api_key=os.getenv(\"DASHSCOPE_API_KEY\"), # å¦‚ä½•è·å–API Keyï¼šhttps://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "#     base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "# )\n",
    "\n",
    "client = OpenAI(\n",
    "   \n",
    "    api_key=\"na\", \n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    ")\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"na\", \n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': 'ä½ å«ä»€ä¹ˆåå­—'}\n",
    "        ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
